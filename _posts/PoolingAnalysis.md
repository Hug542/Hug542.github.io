---
title: 'Pooling Analysis'
date: 2020-05-01
permalink: /posts/Pooling-Analysis/
tags:
  - max-attention
  - vanishing gradients
  - positional biases
---

Why and when should you pool?  <br /> Analyzing Pooling in Recurrent Architectures  
======

TL;DR:
------
1. Pooling-based BiLSTMs demonstrate improved learning ability and positional invariance
2. Pooling helps improve sample efficiency (low-resource settings) and is particularly beneficial when important words lie towards the middle of the sentence
3. Our proposed pooling technique max-attention (MaxAtt) helps improve upon past approaches and is more robust to distribution shift

